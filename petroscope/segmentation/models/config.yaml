hydra:
  job:
    chdir: True

model_type: "hrnet"

# Model-specific settings
model:

  # ResUNet specific parameters
  resunet:
    layers: 4                  # Number of layers in the network (only used if no backbone)
    filters: 24                # Number of starting filters (only used if no backbone)
    backbone: resnet34         # Options: null, resnet18, resnet34, resnet50, resnet101, resnet152
    dilated: true              # Whether to use dilated convolutions
    pretrained: true           # Whether to use pretrained backbone weights
  
  # PSPNet specific parameters
  pspnet:
    backbone: "resnet34"
    dilated: True
      
  # HRNetV2+OCR specific parameters
  hrnet:
    backbone: "hrnetv2_w18"    # Options: hrnetv2_w18, hrnetv2_w32, hrnetv2_w48
    pretrained: True           # Whether to use pretrained backbone weights
    ocr_mid_channels: 512      # Number of channels in OCR module
    dropout: 0.1               # Dropout rate for the model
    use_aux_head: True         # Whether to use auxiliary segmentation head during training

# Training settings
train:
  epochs: 50
  patch_size: 384
  batch_size: 64
  LR: 0.001
  scheduler_patience: 4
  val_steps: 30
  test_every: 1
  amp: False
  gradient_clipping: 1.0

  # Loss configuration
  loss:
    # Class weight configuration for handling class imbalance
    class_weights:
      enabled: false  # Set to true to enable class weights
      method: "inverse"  # Options: inverse, sqrt_inverse, log_inverse, quadratic_inverse
      smooth_factor: 1.0  # Smoothing factor for weight calculations
    
    # Loss type configuration
    # Options: crossentropy, focal, dice, combined
    type: "crossentropy"  
    
    # Parameters for different loss types (used when type is not "combined")
    crossentropy:
      reduction: "mean"
      ignore_index: 255
      label_smoothing: 0.0
    
    focal:
      gamma: 2.0
      reduction: "mean"
      ignore_index: 255
    
    dice:
      smooth: 1.0
      reduction: "mean"
      ignore_index: 255
      include_background: true
    
    # Configuration for combined loss
    components:
      - name: "dice"
        weight: 0.6
        params:
          smooth: 1.0
          reduction: "mean"
          include_background: true
      - name: "focal"
        weight: 0.4
        params:
          gamma: 2.0
          reduction: "mean"

  augm:
    rotation: 20
    scale: 0.1
    brightness: 0.01
    color: 0.02
    factor: 10

  balancer:
    enabled: false
    class_area_consideration: 1.5
    patch_positioning_accuracy: 0.8
    balancing_strength: 0.75
    void_border_width: 4
    acceleration: 8
  
  # Void rare classes configuration for training
  void_rare_classes:
    enabled: false
    class_codes: []  # List of class codes to exclude from training
                     # NOTE: These must be subset of classes defined by data.classes
                     # Example: if data.classes="S1_S2" defines [0,1,2,3,4,5,6,7,8,11]
                     # and void_rare_classes=[8,11], then training uses [0,1,2,3,4,5,6,7]
                     # All other classes (9,10,12-49) are automatically ignored
  
  data_loader:
    num_workers: 4
    prefetch_factor: 4
    pin_memory: true

# Data settings
data:
  # classes: "auto" (detect from dataset) or predefined sets: "S1", "S2", "S3", "S1_S2"
  classes: "auto"
  cache_path: "${oc.env:HOME}/.cache/petroscope-data"
  dataset_path: "${oc.env:HOME}/dev/LumenStone/S1v2_S2v2_x05/"

# Test settings
test:
  void_pad: 4
  void_border_width: 2
  
  # Void rare classes configuration for testing
  void_rare_classes:
    enabled: false
    class_codes: []  # List of class codes to void during testing
                     # NOTE: Should usually match training void_rare_classes
                     # Only classes defined in data.classes can be voided
                     # All undefined classes (not in ClassSet) are automatically ignored
  
  vis_segmentation: true
  max_epoch_visualizations: 15  # Keep visualizations for only the last N epochs

# Hardware settings
hardware:
  device: "cuda:3"
  seed: 40
